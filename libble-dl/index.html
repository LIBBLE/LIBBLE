<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>LIBBLE</title>
    <meta name="description" content="A Library for Big Learning">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:5001/libble-dl/">
    <link rel="alternate" type="application/rss+xml" title="LIBBLE" href="http://localhost:5001/feed.xml" />
	<link type="image/x-icon" rel="shortcut icon" href="/images/icon.png" />
    <!-- <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>-->
	<!-- mathjax config similar to math.stackexchange -->
<style>
	/* cyrillic-ext */
@font-face {
  font-family: 'EB Garamond';
  font-style: normal;
  font-weight: 400;
  src: local('EB Garamond'), local('EBGaramond'), url(https://fonts.gstatic.com/s/ebgaramond/v7/kYZt1bJ8UsGAPRGnkXPeFTTOQ_MqJVwkKsUn0wKzc2I.woff2) format('woff2');
  unicode-range: U+0460-052F, U+20B4, U+2DE0-2DFF, U+A640-A69F;
}
/* cyrillic */
@font-face {
  font-family: 'EB Garamond';
  font-style: normal;
  font-weight: 400;
  src: local('EB Garamond'), local('EBGaramond'), url(https://fonts.gstatic.com/s/ebgaramond/v7/kYZt1bJ8UsGAPRGnkXPeFTUj_cnvWIuuBMVgbX098Mw.woff2) format('woff2');
  unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
}
/* vietnamese */
@font-face {
  font-family: 'EB Garamond';
  font-style: normal;
  font-weight: 400;
  src: local('EB Garamond'), local('EBGaramond'), url(https://fonts.gstatic.com/s/ebgaramond/v7/kYZt1bJ8UsGAPRGnkXPeFb6up8jxqWt8HVA3mDhkV_0.woff2) format('woff2');
  unicode-range: U+0102-0103, U+1EA0-1EF9, U+20AB;
}
/* latin-ext */
@font-face {
  font-family: 'EB Garamond';
  font-style: normal;
  font-weight: 400;
  src: local('EB Garamond'), local('EBGaramond'), url(https://fonts.gstatic.com/s/ebgaramond/v7/kYZt1bJ8UsGAPRGnkXPeFSYE0-AqJ3nfInTTiDXDjU4.woff2) format('woff2');
  unicode-range: U+0100-024F, U+1E00-1EFF, U+20A0-20AB, U+20AD-20CF, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
  font-family: 'EB Garamond';
  font-style: normal;
  font-weight: 400;
  src: local('EB Garamond'), local('EBGaramond'), url(https://fonts.gstatic.com/s/ebgaramond/v7/kYZt1bJ8UsGAPRGnkXPeFY4P5ICox8Kq3LLUNMylGO4.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215;
}
</style>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>


  <body>
  <div class="page">
    <header class="site-header">
    <a class="site-title" href="/">LIBBLE</a>
	<a class="site-description"> A Library for Big Learning</a>
    <nav class="site-nav">
        <a href="#" class="menu-icon menu.open">
            <svg viewBox="0 0 18 15">
                <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
                <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
                <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
        </a>
        <div class="trigger"><h1>Main Navigation</h1>
            <ul class="menu">
    
    
     <li><a href="/" class="page-link">HOME</a>
    
    </li>
    
    
    <li><a href="/libble-spark/" class="page-link">LIBBLE-Spark</a>
    <ul class="sub-menu">
    
    <li><a href="/libble-spark/#introduction">Introduction</a></li>
    
    <li><a href="/libble-spark/#empirical-comparison">Empirical Comparison</a></li>
    
    <li><a href="/libble-spark/#tutorial">Tutorial</a></li>
    
    <li><a href="/libble-spark/#open-source">Open Source</a></li>
    
    <li><a href="/libble-spark/#api">API</a></li>
    
    <li><a href="/libble-spark/#development-team">Development Team</a></li>
    
    </ul>
    
    </li>
    
    
    <li><a href="/libble-ps/" class="page-link">LIBBLE-PS</a>
    <ul class="sub-menu">
    
    <li><a href="/libble-ps/#introduction">Introduction</a></li>
    
    <li><a href="/libble-ps/#empirical-comparison">Empirical Comparison</a></li>
    
    <li><a href="/libble-ps/#how-to-use">How to use</a></li>
    
    <li><a href="/libble-ps/#open-source">Open Source</a></li>
    
    <li><a href="/libble-ps/#development-team">Development Team</a></li>
    
    </ul>
    
    </li>
    
    
    <li><a href="/libble-dl/" class="page-link">LIBBLE-DL</a>
    <ul class="sub-menu">
    
    <li><a href="/libble-dl/#introduction">Introduction</a></li>
    
    <li><a href="/libble-dl/#tutorial">Tutorial</a></li>
    
    <li><a href="/libble-dl/#configure-environment">Configure Environment</a></li>
    
    <li><a href="/libble-dl/#examples">Examples</a></li>
    
    <li><a href="/libble-dl/#empirical-comparison">Empirical Comparison</a></li>
    
    <li><a href="/libble-dl/#open-source">Open Source</a></li>
    
    <li><a href="/libble-dl/#development-team">Development Team</a></li>
    
    </ul>
    
    </li>
    
    
     <li><a href="/publications/" class="page-link">Publications</a>
    
    </li>
    
    
    <li><a href="/downloads/" class="page-link">Downloads</a>
    <ul class="sub-menu">
    
    <li><a href="/downloads/#libble-spark">LIBBLE-Spark</a></li>
    
    </ul>
    
    </li>
    
    
     <li><a href="/news/" class="page-link">News</a>
    
    </li>
    
</ul>

        </div>
    </nav>
</header>


    <div class="post">
    <header class="post-header">
        <h1></h1>
    </header>

    <h2 id="libble-dl">LIBBLE-DL</h2>

<h3 id="introduction"><a href="#introduction">Introduction</a></h3>

<p>LIBBLE-DL is the LIBBLE variant implemented on Pytorch.</p>

<p>PyTorch is a deep learning framework for fast, flexible experimentation.However，PyTorch only provides an MPI-like interface for exchanging tensor data across multi-machine networks.</p>

<p>To handle different kinds of application scenarios, we design and develop three distributed deep learning frameworks on PyTorch: MR-DisPyTorch, RA-DisPyTorch and PS-DisPyTorch. Users can choose suitable framework according to their specific need in real applications.</p>

<h3 id="tutorial"><a href="#tutorial">Tutorial</a></h3>

<ul>
  <li>
    <p>MR-DisPyTorch</p>

    <p>We design and implement a distributed deep learning framework MR-DisPyTorch based on MapReduce programming model <a href="http://delivery.acm.org/10.1145/1330000/1327492/p107-dean.pdf?ip=36.152.24.174&amp;id=1327492&amp;acc=ACTIVE%20SERVICE&amp;key=BF85BBA5741FDC6E%2E180A41DAF8736F97%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1536143614_50ab5ade242506c684bd4bae3902dc93">[Dean et al., 2008]</a>. MR-DisPyTorch adopts a synchronous update strategy. MR-DisPyTorch is able to handle the application scenarios where the network model is small, the number of distributed nodes are small and the computing sources of nodes are even.</p>

    <p>The source code is stored inside the <code class="highlighter-rouge">src/mapreduce</code> directory. We define two classes: <code class="highlighter-rouge">master</code> and <code class="highlighter-rouge">worker</code>, which can make users create nodes conveniently. The instances of them represent master node and worker node, respectively. Users can start a distributed deep learning task with MapReduce programming model by following the code below:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">from</span> <span class="nn">src.mapreduce.master</span> <span class="kn">import</span> <span class="n">master</span>
<span class="kn">from</span> <span class="nn">src.mapreduce.worker</span> <span class="kn">import</span> <span class="n">worker</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">master</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">worker</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span> 
<span class="n">n</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>    </div>
    <p>Parameters of <code class="highlighter-rouge">master</code> and <code class="highlighter-rouge">worker</code> are listed as following:</p>

    <table>
      <thead>
        <tr>
          <th>Parameter</th>
          <th>Comments</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><code class="highlighter-rouge">rank</code></td>
          <td>rank of current process</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">num_workers</code></td>
          <td>number of worker nodes</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">cuda</code></td>
          <td>if you need compute on gpu or not</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">save_path</code></td>
          <td>path to save model (default: none)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">data_loader</code></td>
          <td>train data loader (only on worker nodes)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">test_loader</code></td>
          <td>test data loader (only on master node)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">model</code></td>
          <td>define model architecture</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">criterion</code></td>
          <td>define loss function</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">optim_fn</code></td>
          <td>define optimizer (only on master node)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">adjut</code></td>
          <td>epochs set the learning rate to the initial LR decayed by $10$ (only on master node)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">num_epochs</code></td>
          <td>number of total epochs to run</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">start_epoch</code></td>
          <td>manual epoch number (useful on restarts)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">bucket_comm</code></td>
          <td>if you need communicate layer by layer or not</td>
        </tr>
        <tr>
          <td> </td>
          <td> </td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>RA-DisPyTorch</p>

    <p>We design and implement a decentralized distributed deep learning framework RA-DisPyTorch based on Ring Allreduce programming model <a href="https://github.com/baidu-research/baidu-allreduce">[Gibiansky et al., 2017]</a>. RA-DisPyTorch adopts a synchronous update strategy. RA-DisPyTorch is able to handle the application scenarios where the network model is large, the number of distributed nodes are large and the computing sources of nodes are even.</p>

    <p>The source code is stored inside the <code class="highlighter-rouge">src/ring</code> directory. There is only one kind of compute node in Ring Allreduce programming model. Therefore, we define <code class="highlighter-rouge">node</code> class, each instance of which represent a compute node. Users can start a distributed deep learning task with Ring Allreduce programming model by following the code below:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">from</span> <span class="nn">src.ring.node</span> <span class="kn">import</span> <span class="n">node</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">node</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span> 
<span class="n">n</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>    </div>
    <p>Parameters of <code class="highlighter-rouge">node</code> are listed as following:</p>

    <table>
      <thead>
        <tr>
          <th>Parameter</th>
          <th>Comments</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><code class="highlighter-rouge">rank</code></td>
          <td>rank of current process</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">world_size</code></td>
          <td>number of processes in the distributed group</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">cuda</code></td>
          <td>if you need compute on gpu or not</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">save_path</code></td>
          <td>path to save model (default: none)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">data_loader</code></td>
          <td>train data loader</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">test_loader</code></td>
          <td>test data loader</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">model</code></td>
          <td>define model architecture</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">criterion</code></td>
          <td>define loss function</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">optim_fn</code></td>
          <td>define optimizer</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">adjut</code></td>
          <td>epochs set the learning rate to the initial LR decayed by $10$</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">num_epochs</code></td>
          <td>number of total epochs to run</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">start_epoch</code></td>
          <td>manual epoch number (useful on restarts)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">bucket_comm</code></td>
          <td>if you need communicate layer by layer or not</td>
        </tr>
        <tr>
          <td> </td>
          <td> </td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>PS-DisPyTorch</p>

    <p>We design and implement a distributed deep learning framework PS-DisPyTorch based on Parameter Server programming model <a href="https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf">[Li et al., 2014]</a>. PS-DisPyTorch supports synchronous, asynchronous and semi-synchronous update strategy asynchronous. Synchronous strategy is able to handle the application scenarios where the deep learning model is medium, the number of distributed nodes are medium and the computing sources of nodes are even. Asynchronous and semi-synchronous strategy are able to handle the application scenarios where the deep learning model is medium, the number of distributed nodes are medium and the computing sources of nodes are not even.</p>

    <p>The source code is stored inside the <code class="highlighter-rouge">src/ps</code> directory. We define three classes: <code class="highlighter-rouge">coordinator</code>, <code class="highlighter-rouge">server</code> and <code class="highlighter-rouge">worker</code>, which’s instances represent coordinator node ,server node and worker node, respectively. Users can start a distributed deep learning task with Parameter Server programming model by following the code below:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">from</span> <span class="nn">src.ps.coordinator</span> <span class="kn">import</span> <span class="n">coordinator</span>
<span class="kn">from</span> <span class="nn">src.ps.server</span> <span class="kn">import</span> <span class="n">server</span>
<span class="kn">from</span> <span class="nn">src.ps.worker</span> <span class="kn">import</span> <span class="n">worker</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">coordinator</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span>
<span class="k">elif</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">rank</span> <span class="o">&lt;=</span> <span class="n">num_servers</span><span class="p">:</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">server</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">n</span> <span class="o">=</span> <span class="n">worker</span><span class="p">(</span><span class="o">**</span><span class="n">args</span><span class="p">)</span>
<span class="n">n</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>    </div>
    <p>Parameters of <code class="highlighter-rouge">coordinator</code>, <code class="highlighter-rouge">server</code> and <code class="highlighter-rouge">worker</code> are listed as following:</p>

    <table>
      <thead>
        <tr>
          <th>Parameter</th>
          <th>Comments</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><code class="highlighter-rouge">rank</code></td>
          <td>rank of current process</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">servers</code></td>
          <td>rank list of server nodes</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">workers </code></td>
          <td>rank list of worker nodes</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">cuda</code></td>
          <td>if you need compute on gpu or not</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">save_path</code></td>
          <td>path to save model (default: none)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">data_loader</code></td>
          <td>train data loader (only on worker nodes)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">test_loader</code></td>
          <td>test data loader (only on coordinator node)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">num_batches </code></td>
          <td>number of baches in an epoch</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">model</code></td>
          <td>define model architecture</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">criterion</code></td>
          <td>define loss function</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">optim_fn</code></td>
          <td>define optimizer (only on server node)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">time_window</code></td>
          <td>maximal delay time (only on server nodes)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">adjut</code></td>
          <td>epochs set the learning rate to the initial LR decayed by $10$ (only on server node)</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">num_epochs</code></td>
          <td>number of total epochs to run</td>
        </tr>
        <tr>
          <td><code class="highlighter-rouge">start_epoch</code></td>
          <td>manual epoch number (useful on restarts)</td>
        </tr>
        <tr>
          <td> </td>
          <td> </td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h3 id="configure-environment"><a href="#configure-environment">Configure Environment</a></h3>
<p>We provide a docker image for quickly configuring.</p>
<ul>
  <li>
    <p>Prerequisites</p>

    <p>The list of prerequisites is described below.</p>

    <ul>
      <li>NVIDIA drivers 375.66</li>
      <li>CUDA 8.0</li>
      <li>cuDNN 7.0</li>
      <li>nvidia-docker 1.0</li>
    </ul>
  </li>
  <li>
    <p>How to use</p>

    <p>Download the tar archive <a href="https://pan.baidu.com/s/1Ye-hDHlLtg_ueNxP3kEnGg"><code class="highlighter-rouge">pytorchmpi_cudnn.tar</code></a> and load image from it:</p>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker load <span class="nt">-i</span> pytorchmpi_cudnn.tar
</code></pre></div>    </div>

    <p>Run <code class="highlighter-rouge">utils/bootmpipytorch.sh</code> to create containers and perform SSH login without password among them:</p>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./bootmpipytorch.sh arg1 arg2
</code></pre></div>    </div>

    <p>The first argument indicates the number of containers you want to create. The second argument indicates the path to bind mount a volume. After run the <code class="highlighter-rouge">.sh</code> file, we will have N containers named from $node0$ to $node${N-1}$. We can enter any one of these containers to do further operations by the following command:</p>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> node<span class="k">${</span><span class="nv">i</span><span class="k">}</span> bash
</code></pre></div>    </div>

    <p>We also provide some <code class="highlighter-rouge">.sh</code> files to do batch operations conveniently. The following commands correspond to starting all containers, stoping all containers and removing all containers, respectively:</p>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./startall.sh N
./stopall.sh N
./rmall.sh N
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="examples"><a href="#examples">Examples</a></h3>
<p>We give examples of using these three distributed deep learning frameworks. We choose $20$-layer $ResNet$ model and $Cifar10$ dataset for our examples. Both the training schedule and hyper-parameter settings are following the practice in <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">[He et al., 2016]</a>.</p>

<p>You can find the complete code inside the <code class="highlighter-rouge">example/cifar10</code> directory. In examples, we use <code class="highlighter-rouge">mpi</code> backend to communicate (Of course you can use other backend only if PyTorch supports it). You can start the example for MapReduce programming model with a master node and $4$ worker nodes using the following command:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mpirun <span class="nt">-n</span> 5 <span class="nt">--hostfile</span> hosts python <span class="nt">-m</span> libble_dl.examples.cifar10.mapreduce
</code></pre></div></div>
<p>The file hosts contains a list of IP addresses, indicates which hosts to start the processes on. The usages of RA-DisPyTorch and PS-DisPyTorch are similar.</p>

<h3 id="empirical-comparison"><a href="#empirical-comparison">Empirical Comparison</a></h3>
<ul>
  <li>Efficiency
To compare efficiency with the method provided by Pytorch, the figure below shows the behaviors of our methods. We choose $20$-layer $ResNet$ model and $Cifar10$ dataset for evaluation. Both the training schedule and hyper-parameter settings are following the practice in <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">[He et al., 2016]</a>. All of these four methods adopt $4$ worker nodes.We can find that our methods all outperform the method provided by Pytorch.
    <div align="center">


<img src="/images/train.png" width="500px" text-align="center" />
<img src="/images/test.png" width="500px" text-align="center" />

</div>
  </li>
  <li>
    <p>Speedup
Under the same experiment settings, we also investigate the speedup of these methods by changing the numbers of worker nodes. The speedup is defined as follows: $speedup=\frac{(time~with~1~worker~node)}{(time~with~x~worker~nodes)}$. And we choose $x=2,4,6$. We find that our methods are closer to the ideal speedup because of less communication cost or less synchronization cost.</p>

    <div align="center">

<img src="/images/dl_speed.png" width="500px" text-align="center" />

</div>
  </li>
</ul>

<h3 id="open-source"><a href="#open-source">Open Source</a></h3>

<ul>
  <li>GitHub URL: <a href="https://github.com/LIBBLE/LIBBLE-DL/">https://github.com/LIBBLE/LIBBLE-DL/</a></li>
  <li>Licence: This project follows <a href="https://www.apache.org/licenses/LICENSE-2.0">Apache Licence 2.0</a></li>
</ul>

<h3 id="development-team"><a href="#development-team">Development Team</a></h3>

<ul>
  <li>Director: <a href="http://cs.nju.edu.cn/lwj/">Wu-Jun Li</a></li>
  <li>Developers: <a href="http://lamda.nju.edu.cn/gaoh/">Hao Gao</a>, <a href="http://lamda.nju.edu.cn/shiyh/">Ying-Hao Shi</a></li>
</ul>


</div>


    <footer class="site-footer">
    A Library for Big Learning<br />

    Using <a href="http://jekyllrb.com/">Jekyll</a>
    
     :: <a href="https://github.com/LIBBLE/LIBBLE-Spark">Website source</a>
    
    
</footer>


    

    </div>
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-93186714-1', 'auto');
  ga('send', 'pageview');

</script>
  </body>

</html>
